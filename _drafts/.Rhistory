traindir <- paste(datadir, "train", sep="/") # Complete the path to train folder
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"),quote"\"")
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"),quote="\"")
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"))
?read.table
View(subject_test)
subject_train <- read.table(paste(traindir, "subject_train.txt", sep="/"), quote="\"")
View(subject_train)
test_x  <- read.table(paste(testdir, "X_test.txt", sep="/"), quote="\"")
datadir  <- "/Users/pelayogonzalez/Desktop/Coursera/Getting_Cleaning_Data/data/run_data" # Path to Data directory
testdir  <- paste(datadir, "test", sep="/") # Complete the path to test folder
traindir <- paste(datadir, "train", sep="/") # Complete the path to train folder
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"")
subject_train <- read.table(paste(traindir, "subject_train.txt", sep="/"), quote="\"")
feature_names <- read.table(paste(datadir, "features.txt", sep="/"),quote="\"")
feature_names <- gsub("^[0-9]+ ", "", feature_names$V1)
train_y <- read.table(paste(traindir, "y_train.txt", sep="/"), quote="\"")
test_y  <- read.table(paste(testdir, "y_test.txt", sep="/"), quote="\"")
train_x <- read.table(paste(traindir, "X_train.txt", sep="/"), quote="\"")
test_x  <- read.table(paste(testdir, "X_test.txt", sep="/"), quote="\"")
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"", colnames = "SubjectNumber")
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"", col.names = "SubjectNumber")
datadir  <- "/Users/pelayogonzalez/Desktop/Coursera/Getting_Cleaning_Data/data/run_data" # Path to Data directory
testdir  <- paste(datadir, "test", sep="/") # Complete the path to test folder
traindir <- paste(datadir, "train", sep="/") # Complete the path to train folder
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"", col.names = "SubjectNumber")
subject_train <- read.table(paste(traindir, "subject_train.txt", sep="/"), quote="\"",col.names = "SubjectNumber")
train_y <- read.table(paste(traindir, "y_train.txt", sep="/"), quote="\"",col.names = "Label")
test_y  <- read.table(paste(testdir, "y_test.txt", sep="/"), quote="\"",col.names = "Label")
train_x <- read.table(paste(traindir, "X_train.txt", sep="/"), quote="\"")
test_x  <- read.table(paste(testdir, "X_test.txt", sep="/"), quote="\"")
features <- read.table(paste(datadir, "features.txt", sep="/"),quote="\"")
activity_labels <- read.table(paste(root_path,"/activity_labels.txt",sep=""), quote="\"")
activity_labels <- read.table(paste(datadir,"/activity_labels.txt",sep=""), quote="\"")
data<-rbind(train_x,test_x)
feature <- rbind(features)
View(data)
Subject <- rbind(subject_test, subject_train)
Label <- rbind(y_test, y_train)
Subject <- rbind(subject_test, subject_train)
Label <- rbind(test_y, train_y)
Subject <- rbind(subject_test, subject_train)
Labelact <- merge(Label, activity_labels, by=1)
View(Labelact)
Label.act<- Label.act[,-1]
Label.act <- merge(Label, activity_labels, by=1)
Label.act<- Label.act[,-1]
Label.act <- merge(Label, activity_labels, by=1)
Label.act<- Label.act[,-1]
View(Labelact)
Label.act <- merge(Label, activity_labels, by=1)
Label.act<- Label.act[,2]
Label.act <- merge(Label, activity_labels, by=1)
Labelact<- Label.act[,2]
Data <- rbind(test_x, train_x)
colnames(DataSet) <- features[,2]
colnames(Data) <- features[,2]
View(Data)
###### Merge info on Subjects, Labels and actual Data to create a big Data set
Dataset <- cbind(Subject, Label.act, Data)
colnames(Data) <- c(as.character(features[,2]))
View(Data)
?grep
mean<-grep("mean()",colnames(Dataset),fixed=TRUE)
sd<-grep("sd()",colnames(Dataset),fixed=TRUE)
sd<-grep("std()",colnames(Dataset),fixed=TRUE)
DataMeanSD <- Dataset[,c(Mean,SD)]
mean<-grep("mean()",colnames(Dataset),fixed=TRUE)
std<-grep("std()",colnames(Dataset),fixed=TRUE)
DataMeanStd <- Dataset[,c(mean,std)]
?melt
# install reshape2 package if dosn't exist
if (!require("reshape2")) {
install.packages("reshape2")
require("reshape2")
}
?melt
MeltData = melt(Dataset, id.var = c("Subject.id", "Label.act"))
AveData = dcast(MeltData, Subject.id + Label.act ~ variable,mean)
datadir  <- "/Users/pelayogonzalez/Desktop/Coursera/Getting_Cleaning_Data/data/run_data" # Path to Data directory
testdir  <- paste(datadir, "test", sep="/") # Complete the path to test folder
traindir <- paste(datadir, "train", sep="/") # Complete the path to train folder
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"", col.names = "Subject.id")
subject_train <- read.table(paste(traindir, "subject_train.txt", sep="/"), quote="\"",col.names = "Subject.id")
train_y <- read.table(paste(traindir, "y_train.txt", sep="/"), quote="\"",col.names = "Label")
test_y  <- read.table(paste(testdir, "y_test.txt", sep="/"), quote="\"",col.names = "Label")
train_x <- read.table(paste(traindir, "X_train.txt", sep="/"), quote="\"")
test_x  <- read.table(paste(testdir, "X_test.txt", sep="/"), quote="\"")
features <- read.table(paste(datadir, "features.txt", sep="/"),quote="\"")
activity_labels <- read.table(paste(datadir,"/activity_labels.txt",sep=""), quote="\"")
# Merge auxiliary files containing labels and subjects ids
Label <- rbind(test_y, train_y)
Subject <- rbind(subject_test, subject_train)
#Use descriptive names to name activities
Label.act <- merge(Label, activity_labels, by=1)
Label.act<- Label.act[,2]
#Merge datasets containing quantitative data
Data <- rbind(test_x, train_x)
colnames(Data) <- c(as.character(features[,2]))
###### Merge info on Subjects, Labels and actual Data to create a big Data set
Dataset <- cbind(Subject, Label.act, Data)
#Pick just variables containing mean and std.dev. for each measurement
mean<-grep("mean()",colnames(Dataset),fixed=TRUE)
std<-grep("std()",colnames(Dataset),fixed=TRUE)
DataMeanStd <- Dataset[,c(mean,std)]
# Prints the data table to a file
write.table(DataMeanStd, paste(datadir,"/TidyData.txt",sep=""), sep = ";")
# install reshape2 package if dosn't exist
if (!require("reshape2")) {
install.packages("reshape2")
require("reshape2")
}
####### Creates a "skinny data set" on the basis of subject and activity
####### Note that there are no measure variables as to collapse dublicate
####### variables in the "features" data set
MeltData = melt(Dataset, id.var = c("Subject.id", "Label.act"))
AveData = dcast(MeltData, Subject.id + Label.act ~ variable,mean)
View(MeltData)
AveData = dcast(MeltData, Subject.id + Label.act ~ variable,mean)
?dcast
AveData = dcast(MeltData, MeltData$Subject.id + MeltData$Label.act ~ variable,mean)
library(reshape2)
AveData = dcast(MeltData, MeltData$Subject.id + MeltData$Label.act ~ variable,mean)
Melt.Data = melt(Dataset, id.var = c("Subject.id", "Label.act"))
Tidy.Data = dcast(Melt.Data, formula = Subject.id + Label.act ~ variable, mean)
?drop
# The purpose of this project is to demonstrate your ability to collect, work with,
# and clean a data set. The goal is to prepare tidy data that can be used for later
# analysis. You will be graded by your peers on a series of yes/no questions related
# to the project. You will be required to submit:
#         1) a tidy data set as described below
#         2) a link to a Github repository with your script for performing the analysis
#         3) a code book that describes the variables, the data, and any transformations
#         or work that you performed to clean up the data called CodeBook.md.
#         4)You should also include a README.md in the repo with your scripts.
#         This repo explains how all of the scripts work and how they are connected.
#
# One of the most exciting areas in all of data science right now is wearable computing
# - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing
# to develop the most advanced algorithms to attract new users. The data linked to from the
# course website represent data collected from the accelerometers from the Samsung Galaxy S
# smartphone. A full description is available at the site where the data was obtained:
#
#         http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
#
# Here are the data for the project:
#
#         https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
#
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
# 2. Extracts only the measurements on the mean and standard deviation for each measurement.
# 3. Uses descriptive activity names to name the activities in the data set
# 4. Appropriately labels the data set with descriptive variable names.
# 5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
#0. READ THE DATA
datadir  <- "/Users/pelayogonzalez/Desktop/Coursera/Getting_Cleaning_Data/data/run_data" # Path to Data directory
testdir  <- paste(datadir, "test", sep="/") # Complete the path to test folder
traindir <- paste(datadir, "train", sep="/") # Complete the path to train folder
subject_test  <- read.table(paste(testdir, "subject_test.txt", sep="/"), quote="\"", col.names = "Subject.id")
subject_train <- read.table(paste(traindir, "subject_train.txt", sep="/"), quote="\"",col.names = "Subject.id")
train_y <- read.table(paste(traindir, "y_train.txt", sep="/"), quote="\"",col.names = "Label")
test_y  <- read.table(paste(testdir, "y_test.txt", sep="/"), quote="\"",col.names = "Label")
train_x <- read.table(paste(traindir, "X_train.txt", sep="/"), quote="\"")
test_x  <- read.table(paste(testdir, "X_test.txt", sep="/"), quote="\"")
features <- read.table(paste(datadir, "features.txt", sep="/"),quote="\"")
activity_labels <- read.table(paste(datadir,"/activity_labels.txt",sep=""), quote="\"")
# Merge auxiliary files containing labels and subjects ids
Label <- rbind(test_y, train_y)
Subject <- rbind(subject_test, subject_train)
#Use descriptive names to name activities
Label.act <- merge(Label, activity_labels, by=1)
Label.act<- Label.act[,2]
#Merge datasets containing quantitative data
Data <- rbind(test_x, train_x)
colnames(Data) <- c(as.character(features[,2]))
###### Merge info on Subjects, Labels and actual Data to create a big Data set
Dataset <- cbind(Subject, Label.act, Data)
#Pick just variables containing mean and std.dev. for each measurement
mean<-grep("mean()",colnames(Dataset),fixed=TRUE)
std<-grep("std()",colnames(Dataset),fixed=TRUE)
DataMeanStd <- Dataset[,c(mean,std)]
# Prints the data table to a file
write.table(DataMeanStd, paste(datadir,"/TidyData.txt",sep=""), sep = ";")
# install reshape2 package if dosn't exist
if (!require("reshape2")) {
install.packages("reshape2")
require("reshape2")
}
#Creates a second, independent tidy data set with the average of each variable
#for each activity and each subject.
Melt.Data = melt(Dataset, id.var = c("Subject.id", "Label.act"))
Tidy.Data = dcast(Melt.Data, formula = Subject.id + Label.act ~ variable, mean)
write.table(Tidy.Data, paste(root_path,"/tidy_avg_data.txt",sep=""), sep = ";")
library(reshape2)
head(mtcars)
#Melting dataframes
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
head(carMelt)
#Casting dataframes
cylData <- dcast(carMelt,cyl ~variable)
cylData
cylData <- dcast(carMelt,cyl ~variable, mean)
duplicated(Melt.Data)
source('~/Desktop/Coursera/Getting_Cleaning_Data/Run_project/run_analysis.R')
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
library(dplyr)
install.packages(dplyr)
swirl()
read.csv(path2csv, stringAsFactors=FALSE)
read.csv(path2csv, stringsAsFactors=FALSE)
mydf <- read.csv(path2csv, stringsAsFactors=FALSE)
dim(mydf)
head()
head(MYDF)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf).
cran <- tbl_df(mydf)
rm("mydf")
cran
Use ?manip to pull up the documentation for these core functions.
?manip
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran,-(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
cran
filter(cran, size >100500, r_os=="linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(NA))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
cran2 <- arrange(cran2,ip_id)
arrange(cran2,ip_id)
arrange(cran2,desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version),ip_id)
cran3 <- select(cran, ip_id,package, size)
cran3
mutate(cran3, size_mb = size/(2^20))
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
library(dplyr)
swirl()
cran <- tbl_df(cran)
cran <- tbl_df(mydf)
rm(mydf)
rm("mydf")
cran
?group_by()
?group_by
by_package <-  group_by(cran, package)
by_package
summarize(by_package,mean(size))
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
submit()
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
filter(pack_sum,count > 679)
top_counts <- filter(pack_sum,count > 679)
top_counts
head(top_counts, 20)
arrange(top_counts,desc(count))
quantile(pack_sum&unique, probs =0.99)
quantile(pack_sum$unique, probs =0.99)
top_unique <- filter(pack_sum, unique < 465)
top_unique <- filter(pack_sum, unique > 465)
top_unique
arrange(top_unique, desc(unique))
by_package <- group_by(cran, package)
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
top_countries <- filter(pack_sum, countries > 60)
result1 <- arrange(top_countries, desc(countries), avg_bytes)
print(result1)
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students, sex, count, -grade)
students2
res <- gather(students2,sex_class,value,-grades)
res <- gather(students2,sex_class,value,-grade)
res <- gather(students2,sex_class,count,-grade)
res
?separate
separate(data = res, col = sex_class, into =c("sex","class"))
submit()
students3
?gather
submit()
?spread
submit()
View(students3)
extract_numeric("class5")
?mutate
?mutate
View(students3)
students3 %>%
gather(class, grade, class1:class5, na.rm = TRUE) %>%
spread(test, grade) %>%
print
submit()
students4
submit()
submit()
submit()
passed
failed
passed <- mutate(passed, status, "passed")
passed <- mutate(passed, status = "passed")
failed <- mutate(failed, status = "failed")
?rbind_list
rbind_list(passed,failed)
sat
submit()
View(sat)
submit()
View(sat)
sat %>%
select(-contains("total")) %>%
gather(part_sex, count, -score_range) %>%
separate(part_sex, into =c("part","sex")) %>%
print
View(sat)
submit()
View(sat)
sat %>%
select(-contains("total")) %>%
gather(part_sex, count, -score_range) %>%
separate(part_sex, into =c("part","sex")) %>%
print
sat %>%
select(-contains("total")) %>%
gather(part_sex, count, -score_range) %>%
separate(part_sex, c("part", "sex")) %>%
group_by(part,sex) %>%
mutate(total = sum(count),
prop = count / total
) %>% print
swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
this_day<- today()
this_day
swirl()
library(swirl)
swirl()
day(this_day)
wday(this_day)
wday(this_day, label = TRUE)
this_moment <- now()
this_moment
hour(this_moment)
try ymd("1989-05-17")
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12,1975")
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
| update(this_moment, hours = 8, minutes = 34, seconds = 55).
update(this_moment, hours = 8, minutes = 34, seconds = 55).
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes =34)
depart
arrive <- nyc + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz("Asia/Hong_Kong")
arrive <- with_tz("Asia/Hong_Kong", arrive)
arrive <- with_tz(arrive,"Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008", tz = "Singapore)
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?new_interval
how_long <- new_interval(last_time,arrive)
Now use as.period(how_long) to see how long it's been.
as.period(how_long)
stopwatch()
library(help=”datasets”)
library(help="datasets")
USArrests
airquality
cars
women
UKDriverDeaths
LifeCycleSavings
ChickWeight
mydf <- ChickWeight
mydf
mydf_tb <- tbl_df(mydf)
library(dplyr)
mydf_tb <- tbl_df(mydf)
mydf
mydf_tb
columns_subset <- select(mydf_tb,chick,weight,Time)
columns_subset <- select(mydf_tb,Chick,Weight,Time)
columns_subset <- select(mydf_tb,Chick,weight,Time)
columns_subset2 <- select(mydf_tb, -Diet)
filter(mydf_tb, Time == 0)
filter(mydf_tb, Time == 0 & Diet==1) #This subset collects only data of chicks following diet 1 at birth
filter(mydf_tb, Time < 6 & Diet==1) #This subset collects only data of chicks following diet 1 when they are younger than 6 days.
filter(mydf_tb, Time <= 6 & Diet==1) #This subset collects only data of chicks following diet 1 when they are 6 days old or younger.
is.na(mydf_tb)
length(is.na(mydf_tb))
mydf_tb <- filter(cran, !is.na(mydf_tb))
mydf_tb <- filter(mydf_tb, !is.na(mydf_tb))
mydf_tb <- filter(mydf_tb, !is.na(weight))
mydf_tb <- filter(mydf_tb, !is.na(weight, Chick))
View(mydf_tb)
library(lattice)
library(datasets)
#Simple scatterplot
xyplot(Ozone~Wind,data = airquality)
airquality <- transform(airquality, Month=factor(Month))
xyplot(Ozone~Wind | Month, data = airquality, layout=c(5,1))
#Lattice panel functions
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each=50)
y <- x + f -f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c("Group 1", "Group 2"))
xyplot(y~x | f, layout = c(2,1))
xyplot(y~x | f, panel = function(x,y,...){
panel.xyplot(x,y,...) #First call the default panel function for xyplot
panel.abline(h=median(y), lty=2)
})
#Lattice panel functions: regression line
xyplot(y~x | f, panel = function(x,y,...){
panel.xyplot(x,y,...) #First call the default panel function for xyplot
panel.lmline(x,y, col=2)
})
---
myjekyllsite = c("http://pelayoarbues.github.io/")
setwd("/Users/pelayogonzalez/jekyll-sites/pelayoarbues.github.io/_drafts")
KnitPost <- function(input, base.url = myjekyllsite) {
require(knitr)
opts_knit$set(base.url = base.url)
fig.path <- paste0("figs/", sub(".Rmd$", "", basename(input)), "/")
opts_chunk$set(fig.path = fig.path)
opts_chunk$set(fig.cap = "center")
render_jekyll()
knit(input, envir = parent.frame())
}
KnitPost("eurostat.Rmd")
myjekyllsite = c("http://pelayoarbues.github.io/")
setwd("/Users/pelayogonzalez/jekyll-sites/pelayoarbues.github.io/_drafts")
KnitPost <- function(input, base.url = myjekyllsite) {
require(knitr)
opts_knit$set(base.url = base.url)
fig.path <- paste0("figs/", sub(".Rmd$", "", basename(input)), "/")
opts_chunk$set(fig.path = fig.path)
opts_chunk$set(fig.cap = "center")
render_jekyll()
knit(input, envir = parent.frame())
}
KnitPost("eurostat.Rmd")
KnitPost("eurostat.Rmd")
